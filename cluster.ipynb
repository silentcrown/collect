{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import pandas as pd \n",
    "import pickle \n",
    "import pynlpir\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "data = xlrd.open_workbook('data/知识点导出_2018-10-24.xlsx')  #读取数据\n",
    "page = len(data.sheets())                             #获取sheet的数量\n",
    "table = data.sheets()[0]                                #打开第一张表\n",
    "nrows = table.nrows                                     #获取总行数\n",
    "ncols = table.ncols                                       #获取总列数   \n",
    "gold_data = []\n",
    "knowledge = []\n",
    "for i in range(1,nrows):\n",
    "    know, ques = table.row_values(i)\n",
    "    gold_data.append(ques)\n",
    "    knowledge.append(know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(501393, 1750)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data = open('data/msgwyeth.txt', 'r').read().split('\\n')\n",
    "print knowledge[0]\n",
    "len(add_data),len(knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(data, knowledge):\n",
    "    questions = []\n",
    "    knowledges = []\n",
    "    for i in range(len(data)):\n",
    "        ques = data[i].split('||')\n",
    "        for que in ques:\n",
    "            if que != '':\n",
    "                questions.append(que)\n",
    "                knowledges.append(knowledge[i])\n",
    "    print questions[0]\n",
    "    print len(questions)\n",
    "    return questions, knowledges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1750, 1750)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold_data), len(knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "亲在么\n",
      "74936\n"
     ]
    }
   ],
   "source": [
    "gold_questions, knowledges = get_question(gold_data, knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74936"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(knowledges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.280 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "/home/works/dl-tools/anaconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-8-5f5a20516a14>\u001b[0m(17)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     16 \u001b[0;31m\u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 17 \u001b[0;31m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     18 \u001b[0;31m\u001b[0mnum_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknowledge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> tfidf_matrix\n",
      "<74936x16439 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 640871 stored elements in Compressed Sparse Row format>\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5f5a20516a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mnum_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknowledge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/tfenv.v1.2/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'return'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/works/dl-tools/anaconda2/envs/tfenv.v1.2/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mdispatch_return\u001b[0;34m(self, frame, arg)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_returning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = gold_questions#list(set(list(data['match'])))\n",
    "len(text)\n",
    "print len(text)\n",
    "def jieba_tokenize(text):\n",
    "    return jieba.lcut(text) \n",
    " \n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=jieba_tokenize, \\\n",
    "lowercase=False)\n",
    "'''\n",
    "tokenizer: 指定分词函数\n",
    "lowercase: 在分词之前将所有的文本转换成小写，因为涉及到中文文本处理，\n",
    "所以最好是False\n",
    "'''\n",
    "text_list = text\n",
    " #需要进行聚类的文本集\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_list)\n",
    "import ipdb;\n",
    "ipdb.set_trace()\n",
    "num_clusters = len(knowledge)\n",
    "print num_clusters\n",
    "max_iter = 100\n",
    "km_cluster = KMeans(n_clusters=num_clusters, max_iter=max_iter, n_init=40, \\\n",
    "                    init='k-means++',n_jobs=20)\n",
    "'''\n",
    "n_clusters: 指定K的值\n",
    "max_iter: 对于单次初始值计算的最大迭代次数\n",
    "n_init: 重新选择初始值的次数\n",
    "init: 制定初始值选择的算法\n",
    "n_jobs: 进程个数，为-1的时候是指默认跑满CPU\n",
    "注意，这个对于单个初始值的计算始终只会使用单进程计算，\n",
    "并行计算只是针对与不同初始值的计算。比如n_init=10，n_jobs=40, \n",
    "服务器上面有20个CPU可以开40个进程，最终只会开10个进程\n",
    "'''\n",
    "#返回各自文本的所被分配到的类索引\n",
    "result = km_cluster.fit_predict(tfidf_matrix)\n",
    " \n",
    "print \"Predicting result: \", result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(result, open('data/iter'+str(max_iter)+'.txt','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pickle.load( open('data/iter'+str(300)+'.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_real = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'request_word':text, 'predict':result, 'real': knowledges})\n",
    "new_df = {}\n",
    "new_df['class_num'] = []\n",
    "new_df['request_words'] = []\n",
    "c = 0\n",
    "count = 0\n",
    "new_texts = []\n",
    "class_ = []\n",
    "num_ = []\n",
    "cat = []\n",
    "for i in df['predict'].unique():\n",
    "    d = list(df[df['predict'] == i]['request_word'])\n",
    "    new_df['class_num'].append(c)\n",
    "    new_df['request_words'].append('||'.join(d))\n",
    "    new_texts.append('||'.join(d))\n",
    "    num_.append(len(d))\n",
    "    class_.append(d[0])\n",
    "    cat.append(i)\n",
    "    count += len(d)\n",
    "    c += 1\n",
    "dff = pd.DataFrame({'cat':cat, 'question':new_texts, 'class':class_, 'num':num_})\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "new_pred = []\n",
    "#use = set()\n",
    "map_ = {}\n",
    "pred = list(dff['cat'])\n",
    "for i in range(len(pred)):\n",
    "    match = list(df[df['predict'] == pred[i]]['real']) #对每个聚类label找出对应问题真实值最多的作为他的label\n",
    "    counter = Counter(match)\n",
    "    all_pred = sorted(counter.items(), key=lambda item:item[1], reverse = True)\n",
    "    #print all_pred  \n",
    "    new_pred.append(all_pred[0][0])\n",
    "    map_[pred[i]] = all_pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['new_pred'] = new_pred\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list(df['predict'])\n",
    "new_pred = []\n",
    "real = list(df['real'])\n",
    "for i in range(len(pred)):\n",
    "    new_pred.append(map_[pred[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = real\n",
    "labels_pred = new_pred\n",
    " \n",
    "print (metrics.adjusted_rand_score(labels_true, labels_pred))\n",
    "print (metrics.adjusted_mutual_info_score(labels_true, labels_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print (metrics.fowlkes_mallows_score(labels_true, labels_pred))\n",
    "print (metrics.homogeneity_score(labels_true, labels_pred))\n",
    "print (metrics.completeness_score(labels_true, labels_pred))\n",
    "print (metrics.v_measure_score(labels_true, labels_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2546977431466231\n",
    "0.4608132384545443"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfenv.v1.2]",
   "language": "python",
   "name": "conda-env-tfenv.v1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
